{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "interpretador_de_leis",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1g9Adb53YZsHdaeMuPwnEzdAoUNRFrCLi",
      "authorship_tag": "ABX9TyOVGaYQzVn8YFJyF2UGrknV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mathufnn/congresso_artificial/blob/master/interpretador_de_leis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMHKX6HgA_Ej",
        "colab_type": "code",
        "outputId": "2cb041ac-e5c4-45d3-ecdd-e1424ae591ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# autentica o uso do drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_7jpp9GMWkV",
        "colab_type": "code",
        "outputId": "80eced1e-65e8-45b4-e529-52af0fad06c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# descomprime as pastas de arquivos na cache do notebook. nao afeta o google drive\n",
        "\n",
        "!unzip /content/drive/My\\ Drive/congresso_artificial/dataset_treinamento_arquivo.zip\n",
        "!unzip /content/drive/My\\ Drive/congresso_artificial/lista_votacoes_xml.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/congresso_artificial/dataset_treinamento_arquivo.zip\n",
            "replace dataset_treinamento/materia10_pag10_ano2010.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  /content/drive/My Drive/congresso_artificial/lista_votacoes_xml.zip\n",
            "replace lista_votacoes_xml/lista_votacoes1991.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b4cc5ddc-60d2-46ca-aac3-caf8cf68c94b",
        "id": "-madtBIpjp7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "# verifica se está usando a GPU \n",
        "# saida esperada: '/device:GPU:0'\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NADV4nYrPpLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "####\n",
        "\n",
        "directory = '/content/dataset_treinamento/'\n",
        "raw_text = \"\"\n",
        "\n",
        "for filename in reversed(os.listdir(directory)):\n",
        "    if filename.endswith(\".txt\"):\n",
        "      with open(directory + filename, 'r') as f:\n",
        "        raw_text += f.read() + \" \"\n",
        "\n",
        "####\n",
        "\n",
        "\n",
        "words = text_to_word_sequence(raw_text, filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n', split=' ', lower=True)\n",
        "\n",
        "MAX_WORDS = 100000\n",
        "words = words[0:MAX_WORDS]\n",
        "\n",
        "# print(\"Amostra do texto: \", end=\"\")\n",
        "# for i in words:\n",
        "#   print(i + \" \", end=\"\")\n",
        "# print(\"\\n\")\n",
        "\n",
        "dict_words = sorted(set(words))\n",
        "\n",
        "n_words = len(words)\n",
        "n_vocab = len(dict_words)\n",
        "print(\"Número de palavras: \", n_words)\n",
        "print(\"Tamanho do vocabulário: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrK7zfXVwT-F",
        "colab_type": "code",
        "outputId": "4d002287-4c8b-4557-d528-762fd5ef7a94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "seq_length = 50\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_words - seq_length, 1):\n",
        "  seq_in = words[i:i + seq_length]\n",
        "  seq_out = words[i + seq_length]\n",
        "  dataX.append([word for word in seq_in])\n",
        "  dataY.append(seq_out)\n",
        "n_patterns = len(dataX)\n",
        "print(\"Número de Instâncias: \", n_patterns)\n",
        "\n",
        "tokenizer = Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(dataX)\n",
        "X = tokenizer.texts_to_sequences(dataX)\n",
        "y = tokenizer.texts_to_sequences(dataY)\n",
        "\n",
        "flat_y = [item for sublist in y for item in sublist]\n",
        "\n",
        "print('Numero x: ', len(X))\n",
        "print('Numero y: ', len(flat_y))\n",
        "\n",
        "X = numpy.reshape(X, (n_patterns, seq_length))\n",
        "y = np_utils.to_categorical(flat_y)\n",
        "\n",
        "hidden_size = 128\n",
        "dropout=0.5\n",
        "model = Sequential()\n",
        "model.add(Embedding(n_vocab, 128, input_length=seq_length))     # diminuida a capacidade do modelo reduzindo a camada de embedding de 1024 para 128\n",
        "model.add(GRU(hidden_size, return_sequences=True, dropout=dropout))\n",
        "model.add(GRU(hidden_size, dropout=dropout))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.fit(X, y, epochs=12, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de Instâncias:  99950\n",
            "Numero x:  99950\n",
            "Numero y:  99950\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/12\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "99950/99950 [==============================] - 87s 867us/step - loss: 6.9663\n",
            "\n",
            "Epoch 00001: loss improved from inf to 6.96627, saving model to weights-improvement-01-6.9663.hdf5\n",
            "Epoch 2/12\n",
            "99950/99950 [==============================] - 85s 854us/step - loss: 6.0515\n",
            "\n",
            "Epoch 00002: loss improved from 6.96627 to 6.05151, saving model to weights-improvement-02-6.0515.hdf5\n",
            "Epoch 3/12\n",
            "99950/99950 [==============================] - 86s 858us/step - loss: 5.4679\n",
            "\n",
            "Epoch 00003: loss improved from 6.05151 to 5.46789, saving model to weights-improvement-03-5.4679.hdf5\n",
            "Epoch 4/12\n",
            "99950/99950 [==============================] - 85s 855us/step - loss: 4.9668\n",
            "\n",
            "Epoch 00004: loss improved from 5.46789 to 4.96684, saving model to weights-improvement-04-4.9668.hdf5\n",
            "Epoch 5/12\n",
            "99950/99950 [==============================] - 85s 852us/step - loss: 4.5560\n",
            "\n",
            "Epoch 00005: loss improved from 4.96684 to 4.55599, saving model to weights-improvement-05-4.5560.hdf5\n",
            "Epoch 6/12\n",
            "99950/99950 [==============================] - 85s 853us/step - loss: 4.2383\n",
            "\n",
            "Epoch 00006: loss improved from 4.55599 to 4.23835, saving model to weights-improvement-06-4.2383.hdf5\n",
            "Epoch 7/12\n",
            "99950/99950 [==============================] - 86s 857us/step - loss: 3.9811\n",
            "\n",
            "Epoch 00007: loss improved from 4.23835 to 3.98114, saving model to weights-improvement-07-3.9811.hdf5\n",
            "Epoch 8/12\n",
            "99950/99950 [==============================] - 85s 849us/step - loss: 3.7658\n",
            "\n",
            "Epoch 00008: loss improved from 3.98114 to 3.76584, saving model to weights-improvement-08-3.7658.hdf5\n",
            "Epoch 9/12\n",
            "99950/99950 [==============================] - 85s 850us/step - loss: 3.5757\n",
            "\n",
            "Epoch 00009: loss improved from 3.76584 to 3.57567, saving model to weights-improvement-09-3.5757.hdf5\n",
            "Epoch 10/12\n",
            "99950/99950 [==============================] - 86s 858us/step - loss: 3.4078\n",
            "\n",
            "Epoch 00010: loss improved from 3.57567 to 3.40775, saving model to weights-improvement-10-3.4078.hdf5\n",
            "Epoch 11/12\n",
            "99950/99950 [==============================] - 86s 858us/step - loss: 3.2629\n",
            "\n",
            "Epoch 00011: loss improved from 3.40775 to 3.26288, saving model to weights-improvement-11-3.2629.hdf5\n",
            "Epoch 12/12\n",
            "99950/99950 [==============================] - 85s 855us/step - loss: 3.1257\n",
            "\n",
            "Epoch 00012: loss improved from 3.26288 to 3.12568, saving model to weights-improvement-12-3.1257.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1228ec57b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z25PwSSWNkD7",
        "outputId": "8d7e7f47-5f90-4336-ec17-63f53930ae8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "\n",
        "#seleciona um trecho aleatório do livro para servir como entrada\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "X = tokenizer.texts_to_sequences([pattern])[0]\n",
        "print('Texto fornecido como entrada: ')\n",
        "print(\"\\\"\", ' '.join([value for value in pattern]), \"\\\"\")\n",
        "print('Continuação do texto gerado pela rede: ')\n",
        "\n",
        "probability = 1.0\n",
        "\n",
        "for i in range(50):\n",
        "  x_in = numpy.reshape(X, (1, seq_length))\n",
        "  prediction = model.predict(x_in, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  probability *= prediction[0][index]\n",
        "\n",
        "  result = tokenizer.sequences_to_texts([[index]])\n",
        "\n",
        "  sys.stdout.write(result[0])\n",
        "  sys.stdout.write(' ')\n",
        "  X.append(index)\n",
        "  X = X[1:len(X)]\n",
        "  \n",
        "print()\n",
        "print('Probabilidade do texto gerado: ', probability)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Texto fornecido como entrada: \n",
            "\" suplemento volume no vii tomo no i v suplemento no 147 publicado no dsf paginas 774 982 pub informacoes complementares suplemento volume no vii tomo no v v suplemento no 147 publicado no dsf paginas 641 771 pub informacoes complementares suplemento volume no vii tomo no iv v suplemento no \"\n",
            "Continuação do texto gerado pela rede: \n",
            "197 publicado no dsf paginas 3 280 pub informacoes complementares suplemento volume no iv tomo no iv suplemento no 197 publicado no dsf paginas 3 280 pub informacoes complementares suplemento volume no iv tomo no iv suplemento no 197 publicado no dsf paginas 3 280 pub informacoes complementares suplemento volume \n",
            "Probabilidade do texto gerado:  1.1639031646911483e-10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSKXuCM5ZKKC",
        "colab_type": "code",
        "outputId": "22dde1b8-0ac8-4a71-d643-b08ddc9f171c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "!pip install xlrd"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.6/dist-packages (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG11yXK9FZdK",
        "colab_type": "code",
        "outputId": "6aff0889-1c28-4a20-ff39-f8f152bcbaf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "# importando os dois outros conjuntos de dados\n",
        "\n",
        "import xlrd     # biblioteca para parsing de arquivos .xlsx\n",
        "\n",
        "class PEC:\n",
        "  def __init__(self, num=\"\", ass=\"\", just=\"\", res=\"\"):\n",
        "    self.numero = num\n",
        "    self.assunto = ass\n",
        "    self.descricao = just\n",
        "    self.resultado = res\n",
        "  def __repr__(self):\n",
        "    return \"Número: %s, Assunto: %s, Resultado: %s, Descricao: %s \\n\" % (self.numero, self.assunto, self.resultado, self.descricao)\n",
        "\n",
        "!cp \"/content/drive/My Drive/congresso_artificial/PECs.xlsx\" \"/content/\"\n",
        "\n",
        "xl_workbook = xlrd.open_workbook(\"/content/PECs.xlsx\")\n",
        "lista_pecs = []\n",
        "\n",
        "for xl_sheet in xl_workbook.sheets():\n",
        "  for row_idx in range(1, xl_sheet.nrows):\n",
        "    temp_pec = []\n",
        "    for col_idx in range(0, xl_sheet.ncols):\n",
        "      temp_pec.append(xl_sheet.cell(row_idx, col_idx).value)\n",
        "    pec = PEC(temp_pec[0], temp_pec[1], temp_pec[2], temp_pec[4])\n",
        "    lista_pecs.append(pec)\n",
        "\n",
        "pd.DataFrame(lista_pecs)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Número: Nº: 141/2015, Assunto: Administrativo ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Número: Nº: 15/2015, Assunto: Jurídico - Direi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Número: Nº 72/2015, Assunto: Econômico - Tribu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Número: Nº 61/2015, Assunto: Econômico - Plane...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Número: Nº 153/2015, Assunto: Social - Meio am...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>Número: 20/2017, Assunto: Social - Saúde, Resu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Número: 29/2017, Assunto: Administrativo - Org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>Número: 33/2017, Assunto: Jurídico - Direito e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>Número: 004/2017, Assunto: Administrativo - Se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>Número: 003/2017, Assunto: Administrativo - Or...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>132 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     0\n",
              "0    Número: Nº: 141/2015, Assunto: Administrativo ...\n",
              "1    Número: Nº: 15/2015, Assunto: Jurídico - Direi...\n",
              "2    Número: Nº 72/2015, Assunto: Econômico - Tribu...\n",
              "3    Número: Nº 61/2015, Assunto: Econômico - Plane...\n",
              "4    Número: Nº 153/2015, Assunto: Social - Meio am...\n",
              "..                                                 ...\n",
              "127  Número: 20/2017, Assunto: Social - Saúde, Resu...\n",
              "128  Número: 29/2017, Assunto: Administrativo - Org...\n",
              "129  Número: 33/2017, Assunto: Jurídico - Direito e...\n",
              "130  Número: 004/2017, Assunto: Administrativo - Se...\n",
              "131  Número: 003/2017, Assunto: Administrativo - Or...\n",
              "\n",
              "[132 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGHoCTqOX7Qq",
        "colab_type": "code",
        "outputId": "2f2147bc-e6b7-4de5-9535-15d54ea0f2c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "class Votacao:\n",
        "  def __init__(self, data=\"\", cod=0, sigla=\"\", casa=\"\", secreta=\"\", res=\"\", votos={}, total_sim=0, total_nao=0, ano=\"\", desc=\"\", desc_ident=\"\"):\n",
        "    self.data_sessao = data       # string formato aaaa-mm-dd\n",
        "    self.codigo_sessao = cod      # inteiro formado por cast de string\n",
        "    self.sigla_materia = sigla    # string, sigla referente a materia votada (Ex. PEC, MSF)\n",
        "    self.casa = casa              # string, referente a casa onde foi votada a sessao\n",
        "    self.secreta = secreta        # string com valores S ou N (secreta ou nao-secreta)\n",
        "    self.resultado = res          # string com valores A ou R (materia aprovada ou reprovada)\n",
        "    self.votos = votos            # dicionario que relaciona numero do senador com o valor do voto (sim, nao, ausente, nao registrou, etc)\n",
        "    self.total_votos_sim = total_sim  # inteiro formado por cast de string\n",
        "    self.total_votos_nao = total_nao  # inteiro formado por cast de string\n",
        "    self.ano_materia = ano        # string em formato aaaa\n",
        "    self.descricao = desc         # string grande com descricao da votacao da materia\n",
        "    self.descricao_identificacao = desc_ident  # string que identifica as materias pelo indice\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return \"Data: %s, Codigo: %s, Resultado: %s \\n\" % (self.data_sessao, self.codigo_sessao, self.resultado)\n",
        "\n",
        "lista_votacoes = []               # lista de instancias da classe Votacao, pode ser exportada para outro arquivo\n",
        "\n",
        "def trata_dado_ausente(elemento):# funcao para tratar objetos do tipo \"NoneType\" que aparecem quando\n",
        "  var = elemento                  # ha dados ausentes, quando o elemento e uma string.\n",
        "  if(elemento is None):           # caso o elemento seja NoneType, atribui-se uma string vazia a ele\n",
        "    var = \"\"\n",
        "  else:                           # caso contrario, ele recebe o conteudo do elemento a que pertence\n",
        "    var = var.text\n",
        "  return var\n",
        "\n",
        "def trata_dado_ausente_int(elemento):  # trata objetos \"NoneType\" e os substitui por numeros\n",
        "  var = elemento\n",
        "  if(elemento is None):\n",
        "    var = 0\n",
        "  else:\n",
        "    var = int(var.text)\n",
        "  return var\n",
        "\n",
        "def limpar_texto(texto):          # trata caracteres com acento e caracteres invalidos.\n",
        "  texto = texto.lower()\n",
        "  nfkd = unicodedata.normalize('NFKD', texto)\n",
        "  palavraSemAcento = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
        "  texto_limpo = re.sub('[^a-zA-Z0-9 \\\\\\]', ' ', palavraSemAcento)\n",
        "  texto_limpo = ' '.join(texto_limpo.split())\n",
        "\n",
        "  return texto_limpo\n",
        "\n",
        "for i in range(1991, 2018):       # itera sobre os anos dos arquivos XML das votacoes e faz o parsing do xml\n",
        "  with open(\"lista_votacoes_xml/lista_votacoes\" + str(i) + \".xml\", 'r', encoding=\"utf-8\") as f:\n",
        "    root = ET.fromstring(f.read())\n",
        "\n",
        "  for votacao_element in root[1].findall(\"Votacao\"):\n",
        "    data1 = trata_dado_ausente(votacao_element.find(\"DataSessao\"))\n",
        "    codigo1 = trata_dado_ausente_int(votacao_element.find(\"CodigoSessao\"))\n",
        "    sigla1 = trata_dado_ausente(votacao_element.find(\"SiglaMateria\"))\n",
        "    casa1 = trata_dado_ausente(votacao_element.find(\"SiglaCasa\"))\n",
        "    secreta1 = trata_dado_ausente(votacao_element.find(\"Secreta\"))\n",
        "    resultado1 = trata_dado_ausente(votacao_element.find(\"Resultado\"))\n",
        "\n",
        "    votos = {}\n",
        "    for voto in votacao_element.iter(\"Votos\"):\n",
        "      codigo_parl = voto.find(\"CodigoParlamentar\")\n",
        "      voto_parl = voto.find(\"Voto\")\n",
        "      votos[codigo_parl] = voto_parl\n",
        "\n",
        "    total_votos_sim1 = trata_dado_ausente_int(votacao_element.find(\"TotalVotosSim\"))\n",
        "    total_votos_nao1 = trata_dado_ausente_int(votacao_element.find(\"TotalVotosNao\"))\n",
        "    ano_materia1 = trata_dado_ausente(votacao_element.find(\"AnoMateria\"))\n",
        "    descricao1 = limpar_texto(trata_dado_ausente(votacao_element.find(\"DescricaoVotacao\")))\n",
        "    descricao_identificacao1 = trata_dado_ausente(votacao_element.find(\"DescricaoIdentificacaoMateria\"))\n",
        "    \n",
        "    instancia_votacao = Votacao(data1, codigo1, sigla1, casa1, secreta1, resultado1, votos, total_votos_sim1, total_votos_sim1, ano_materia1, descricao1)\n",
        "    lista_votacoes.append(instancia_votacao)    # atribui os valores do arquivo XML as instancias da classe\n",
        "\n",
        "pd.DataFrame(lista_votacoes)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Data: 1991-03-05, Codigo: 20277, Resultado: A \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data: 1991-03-05, Codigo: 20277, Resultado: A \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Data: 1991-03-06, Codigo: 20278, Resultado: A \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Data: 1991-03-06, Codigo: 20278, Resultado: A \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Data: 1991-03-06, Codigo: 20278, Resultado: A \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3195</th>\n",
              "      <td>Data: 2017-12-13, Codigo: 57023, Resultado:  \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3196</th>\n",
              "      <td>Data: 2017-12-13, Codigo: 57023, Resultado: A \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3197</th>\n",
              "      <td>Data: 2017-12-13, Codigo: 57023, Resultado: A \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3198</th>\n",
              "      <td>Data: 2017-12-13, Codigo: 57023, Resultado: A \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3199</th>\n",
              "      <td>Data: 2017-12-14, Codigo: 57256, Resultado:  \\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3200 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     0\n",
              "0     Data: 1991-03-05, Codigo: 20277, Resultado: A \\n\n",
              "1     Data: 1991-03-05, Codigo: 20277, Resultado: A \\n\n",
              "2     Data: 1991-03-06, Codigo: 20278, Resultado: A \\n\n",
              "3     Data: 1991-03-06, Codigo: 20278, Resultado: A \\n\n",
              "4     Data: 1991-03-06, Codigo: 20278, Resultado: A \\n\n",
              "...                                                ...\n",
              "3195   Data: 2017-12-13, Codigo: 57023, Resultado:  \\n\n",
              "3196  Data: 2017-12-13, Codigo: 57023, Resultado: A \\n\n",
              "3197  Data: 2017-12-13, Codigo: 57023, Resultado: A \\n\n",
              "3198  Data: 2017-12-13, Codigo: 57023, Resultado: A \\n\n",
              "3199   Data: 2017-12-14, Codigo: 57256, Resultado:  \\n\n",
              "\n",
              "[3200 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2sprIcIj3kj",
        "colab_type": "code",
        "outputId": "e02313da-6206-4e6a-9f70-4a1f35465c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# criando dois novos conjuntos de dados. um com votacoes do senado e outras com PECs. cada uma tem target 0 ou 1, significando se foram aprovadas ou nao.\n",
        "import random\n",
        "\n",
        "data1 = [] # votacoes\n",
        "data2 = [] # pecs\n",
        "\n",
        "for votacao in lista_votacoes:\n",
        "  if votacao.resultado == \"A\":\n",
        "    data1.append([(votacao.descricao), 1])\n",
        "  elif votacao.resultado == \"R\":\n",
        "    data1.append([(votacao.descricao), 0])\n",
        "\n",
        "random.shuffle(data1)\n",
        "data1_train = data1[ : int(len(data1)*0.8)]\n",
        "data1_test = data1[int(len(data1)*0.8) : ]\n",
        "\n",
        "print(len(data1_train), len(data1_test))\n",
        "\n",
        "for pec in lista_pecs:\n",
        "  if \"aprovada\" in pec.resultado.lower():\n",
        "    descricao_pec = limpar_texto(pec.descricao)\n",
        "    data2.append([descricao_pec, 1])\n",
        "  elif \"arquivada\" in pec.resultado.lower():\n",
        "    resultado_pec = limpar_texto(pec.resultado)\n",
        "    data2.append([pec.resultado, 0])\n",
        "\n",
        "random.shuffle(data2)\n",
        "data2_train = data2[ : int(len(data2)*0.8)]\n",
        "data2_test = data2[int(len(data2)*0.8) : ]\n",
        "\n",
        "print(len(data2_train), len(data2_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2440 610\n",
            "100 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3L-0VG1Zbuw",
        "colab_type": "code",
        "outputId": "c7165d2c-d7ac-4a4a-8a8e-ff8032b9d3e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "# alterando a rede neural ja treinada para treinar no novo conjunto de dados.\n",
        "\n",
        "new_model = model\n",
        "\n",
        "for layer in new_model.layers[:3]:    # faz com que as camadas de embedding e de GRUs fiquem congeladas.\n",
        "    layer.trainable = False\n",
        "\n",
        "new_model.pop()\n",
        "new_model.add(Dense(2, activation='softmax'))   # substitui a camada de saida por uma camada binaria, em vez de um vetor de palavras.\n",
        "new_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "new_model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 1024)          10743808  \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 50, 128)           442752    \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 128)               98688     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 11,318,530\n",
            "Trainable params: 33,282\n",
            "Non-trainable params: 11,285,248\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXQhexwq9EDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data2_train_x = [i[0] for i in data1_train]\n",
        "# for frase in data2_train_x:\n",
        "#   frase = text_to_word_sequence(frase, filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n', split=' ', lower=False)\n",
        "#   print(frase)\n",
        "# print(data2_train_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK1ZAbgEv0f6",
        "colab_type": "code",
        "outputId": "bdfd8f7e-ac18-41e2-a9f1-f1c731a28f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "data1_train_x = [i[0] for i in data1_train]\n",
        "data1_train_y = [i[1] for i in data1_train]\n",
        "\n",
        "data2_train_x = [i[0] for i in data2_train]\n",
        "data2_train_y = [i[1] for i in data2_train]\n",
        "\n",
        "# separando as strings inteiras em listas de palavras, e em seguida transformando em tokens\n",
        "temp = []\n",
        "for frase in data1_train_x:\n",
        "  frase_lista = text_to_word_sequence(frase, filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n', split=' ', lower=False)\n",
        "  temp.append(frase_lista)\n",
        "data1_train_x = temp\n",
        "\n",
        "temp = []\n",
        "for frase in data2_train_x:\n",
        "  frase_lista = text_to_word_sequence(frase, filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n', split=' ', lower=False)\n",
        "  temp.append(frase_lista)\n",
        "data2_train_x = temp\n",
        "\n",
        "tokenizer = Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(data1_train_x)\n",
        "tokenizer.fit_on_texts(data2_train_x)\n",
        "data1_train_x = tokenizer.texts_to_sequences(data1_train_x)\n",
        "data2_train_x = tokenizer.texts_to_sequences(data2_train_x)\n",
        "data1_train_x = np.array(data1_train_x)\n",
        "data1_train_x = np.nan_to_num(data1_train_x)\n",
        "data2_train_x = np.array(data2_train_x)\n",
        "\n",
        "pd.DataFrame(data1_train_x)\n",
        "\n",
        "# PROBLEMA: a descricao de PECs, quando vetorizada, fica com dimensoes irregulares pois as descricoes possuem tamanhos diferentes.\n",
        "\n",
        "# print(data1_train_x.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[10, 2, 12, 158, 1, 149, 1117, 1118, 7, 11, 6,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[71, 55, 47, 5, 102, 17, 15, 1, 26, 2, 61, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[8, 57, 4, 36, 2, 12, 591, 2, 503, 149, 28, 1,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[8, 2, 101, 350, 31, 42, 42, 13, 54, 2, 31, 42...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[10, 2, 167, 65, 1, 242, 657, 7, 35, 6, 60, 50...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2435</th>\n",
              "      <td>[8, 4, 37, 3, 608, 1, 85, 10, 1, 72, 1, 75, 78...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2436</th>\n",
              "      <td>[8, 4, 175, 288, 1, 85, 10, 1, 72, 1, 75, 78, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2437</th>\n",
              "      <td>[14, 3, 204, 55, 104, 4, 3802, 14, 17, 3803, 8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2438</th>\n",
              "      <td>[10, 2, 167, 1833, 1834, 1, 242, 13, 197, 174,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2439</th>\n",
              "      <td>[74, 36, 2, 12, 575, 495, 1, 149, 328, 7, 11, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2440 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      0\n",
              "0     [10, 2, 12, 158, 1, 149, 1117, 1118, 7, 11, 6,...\n",
              "1     [71, 55, 47, 5, 102, 17, 15, 1, 26, 2, 61, 3, ...\n",
              "2     [8, 57, 4, 36, 2, 12, 591, 2, 503, 149, 28, 1,...\n",
              "3     [8, 2, 101, 350, 31, 42, 42, 13, 54, 2, 31, 42...\n",
              "4     [10, 2, 167, 65, 1, 242, 657, 7, 35, 6, 60, 50...\n",
              "...                                                 ...\n",
              "2435  [8, 4, 37, 3, 608, 1, 85, 10, 1, 72, 1, 75, 78...\n",
              "2436  [8, 4, 175, 288, 1, 85, 10, 1, 72, 1, 75, 78, ...\n",
              "2437  [14, 3, 204, 55, 104, 4, 3802, 14, 17, 3803, 8...\n",
              "2438  [10, 2, 167, 1833, 1834, 1, 242, 13, 197, 174,...\n",
              "2439  [74, 36, 2, 12, 575, 495, 1, 149, 328, 7, 11, ...\n",
              "\n",
              "[2440 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVkw2xKasEtK",
        "colab_type": "code",
        "outputId": "87ad7271-e3eb-4629-fc65-b3d6e7ed4a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "new_model.fit(data1_train_x, data1_train_y, epochs=12, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d86352104f73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata1_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_1_input to have shape (50,) but got array with shape (1,)"
          ]
        }
      ]
    }
  ]
}