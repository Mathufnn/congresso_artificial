{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "interpretador_de_leis",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNMSMNAmQPmx+EEiW3icMPt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mathufnn/congresso_artificial/blob/master/interpretador_de_leis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMHKX6HgA_Ej",
        "colab_type": "code",
        "outputId": "b0919c79-7e34-4391-f16a-937dc5a54594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# autentica o uso do drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_7jpp9GMWkV",
        "colab_type": "code",
        "outputId": "4809d2dc-3934-4acd-a1b3-e4dc9a2ded68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# descomprime as pastas de arquivos na cache do notebook. nao afeta o google drive\n",
        "\n",
        "!unzip /content/drive/My\\ Drive/congresso_artificial/dataset_treinamento_arquivo.zip\n",
        "!unzip /content/drive/My\\ Drive/congresso_artificial/lista_votacoes_xml.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/congresso_artificial/dataset_treinamento_arquivo.zip\n",
            "replace dataset_treinamento/materia10_pag10_ano2010.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  /content/drive/My Drive/congresso_artificial/lista_votacoes_xml.zip\n",
            "replace lista_votacoes_xml/lista_votacoes1991.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-29k0jJHHaM",
        "colab_type": "code",
        "outputId": "8d17c396-d05d-464d-bda8-9ccede4ed3b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "# verifica se está usando a GPU \n",
        "# saida esperada: '/device:GPU:0'\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NADV4nYrPpLt",
        "colab_type": "code",
        "outputId": "8e048ecd-9ed7-4cf1-a299-165b9611de2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "####\n",
        "\n",
        "directory = '/content/dataset_treinamento/'\n",
        "raw_text = \"\"\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".txt\"):\n",
        "      with open(directory + filename, 'r') as f:\n",
        "        raw_text += f.read() + \" \"\n",
        "\n",
        "####\n",
        "\n",
        "words = text_to_word_sequence(raw_text, filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n', split=' ', lower=True)\n",
        "\n",
        "MAX_WORDS = 100000\n",
        "words = words[0:MAX_WORDS]\n",
        "\n",
        "dict_words = sorted(set(words))\n",
        "\n",
        "n_words = len(words)\n",
        "n_vocab = len(dict_words)\n",
        "print(\"Número de palavras: \", n_words)\n",
        "print(\"Tamanho do vocabulário: \", n_vocab)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de palavras:  100000\n",
            "Tamanho do vocabulário:  9955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrK7zfXVwT-F",
        "colab_type": "code",
        "outputId": "fe23b263-77f6-4bb3-e17e-239790f151da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# filename = \"dom_casmurro.txt\"\n",
        "# raw_text = open(filename).read()\n",
        "# raw_text = raw_text.lower()\n",
        "\n",
        "# punch = ['.', '[', ']', '(', ')', ';', ':', \"'\", '/', '\"', ',', '?', '*', '!', '-', '$', '%', '&', '\\n']\n",
        "\n",
        "# for i in punch:    \n",
        "#     raw_text = raw_text.replace(i, ' ' + i + ' ')\n",
        "\n",
        "# words = text_to_word_sequence(raw_text, filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n', split=' ', lower=True)\n",
        "# dict_words = sorted(set(words))\n",
        "\n",
        "# print(\"Número de palavras: \", n_words)\n",
        "# print(\"Tamanho do vocabulário: \", n_vocab)\n",
        "\n",
        "seq_length = 50\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_words - seq_length, 1):\n",
        "  seq_in = words[i:i + seq_length]\n",
        "  seq_out = words[i + seq_length]\n",
        "  dataX.append([word for word in seq_in])\n",
        "  dataY.append(seq_out)\n",
        "n_patterns = len(dataX)\n",
        "print(\"Número de Instâncias: \", n_patterns)\n",
        "\n",
        "print(\"haha1\") # metodo avancado de debugging\n",
        "\n",
        "tokenizer = Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(dataX)\n",
        "X = tokenizer.texts_to_sequences(dataX)\n",
        "y = tokenizer.texts_to_sequences(dataY)\n",
        "\n",
        "print(\"haha2\")\n",
        "\n",
        "flat_y = [item for sublist in y for item in sublist]\n",
        "\n",
        "print('Numero x: ', len(X))\n",
        "print('Numero y: ', len(flat_y))\n",
        "\n",
        "print(\"haha3\")\n",
        "\n",
        "X = numpy.reshape(X, (n_patterns, seq_length))\n",
        "y = np_utils.to_categorical(flat_y)\n",
        "\n",
        "print(\"haha4\")\n",
        "\n",
        "hidden_size = 128\n",
        "dropout=0.6\n",
        "model = Sequential()\n",
        "model.add(Embedding(n_vocab, 1024, input_length=seq_length))\n",
        "model.add(GRU(hidden_size, return_sequences=True, dropout=dropout))\n",
        "model.add(GRU(hidden_size, dropout=dropout))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "print(\"haha5\")\n",
        "\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.fit(X, y, epochs=25, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de Instâncias:  99950\n",
            "haha1\n",
            "haha2\n",
            "Numero x:  99950\n",
            "Numero y:  99950\n",
            "haha3\n",
            "haha4\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "haha5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/25\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "99950/99950 [==============================] - 92s 922us/step - loss: 6.9517\n",
            "\n",
            "Epoch 00001: loss improved from inf to 6.95168, saving model to weights-improvement-01-6.9517.hdf5\n",
            "Epoch 2/25\n",
            "99950/99950 [==============================] - 82s 820us/step - loss: 6.2506\n",
            "\n",
            "Epoch 00002: loss improved from 6.95168 to 6.25060, saving model to weights-improvement-02-6.2506.hdf5\n",
            "Epoch 3/25\n",
            "99950/99950 [==============================] - 83s 827us/step - loss: 5.7576\n",
            "\n",
            "Epoch 00003: loss improved from 6.25060 to 5.75758, saving model to weights-improvement-03-5.7576.hdf5\n",
            "Epoch 4/25\n",
            "99950/99950 [==============================] - 82s 825us/step - loss: 5.3953\n",
            "\n",
            "Epoch 00004: loss improved from 5.75758 to 5.39526, saving model to weights-improvement-04-5.3953.hdf5\n",
            "Epoch 5/25\n",
            "99950/99950 [==============================] - 83s 826us/step - loss: 5.1055\n",
            "\n",
            "Epoch 00005: loss improved from 5.39526 to 5.10552, saving model to weights-improvement-05-5.1055.hdf5\n",
            "Epoch 6/25\n",
            "99950/99950 [==============================] - 82s 824us/step - loss: 4.8670\n",
            "\n",
            "Epoch 00006: loss improved from 5.10552 to 4.86702, saving model to weights-improvement-06-4.8670.hdf5\n",
            "Epoch 7/25\n",
            "99950/99950 [==============================] - 83s 827us/step - loss: 4.6639\n",
            "\n",
            "Epoch 00007: loss improved from 4.86702 to 4.66386, saving model to weights-improvement-07-4.6639.hdf5\n",
            "Epoch 8/25\n",
            "99950/99950 [==============================] - 83s 826us/step - loss: 4.4756\n",
            "\n",
            "Epoch 00008: loss improved from 4.66386 to 4.47564, saving model to weights-improvement-08-4.4756.hdf5\n",
            "Epoch 9/25\n",
            "99950/99950 [==============================] - 83s 827us/step - loss: 4.3063\n",
            "\n",
            "Epoch 00009: loss improved from 4.47564 to 4.30629, saving model to weights-improvement-09-4.3063.hdf5\n",
            "Epoch 10/25\n",
            "99950/99950 [==============================] - 82s 821us/step - loss: 4.1537\n",
            "\n",
            "Epoch 00010: loss improved from 4.30629 to 4.15369, saving model to weights-improvement-10-4.1537.hdf5\n",
            "Epoch 11/25\n",
            "99950/99950 [==============================] - 82s 824us/step - loss: 4.0155\n",
            "\n",
            "Epoch 00011: loss improved from 4.15369 to 4.01549, saving model to weights-improvement-11-4.0155.hdf5\n",
            "Epoch 12/25\n",
            "99950/99950 [==============================] - 82s 818us/step - loss: 3.8826\n",
            "\n",
            "Epoch 00012: loss improved from 4.01549 to 3.88261, saving model to weights-improvement-12-3.8826.hdf5\n",
            "Epoch 13/25\n",
            "99950/99950 [==============================] - 82s 824us/step - loss: 3.7617\n",
            "\n",
            "Epoch 00013: loss improved from 3.88261 to 3.76167, saving model to weights-improvement-13-3.7617.hdf5\n",
            "Epoch 14/25\n",
            "99950/99950 [==============================] - 83s 827us/step - loss: 3.6450\n",
            "\n",
            "Epoch 00014: loss improved from 3.76167 to 3.64498, saving model to weights-improvement-14-3.6450.hdf5\n",
            "Epoch 15/25\n",
            "99950/99950 [==============================] - 83s 827us/step - loss: 3.5463\n",
            "\n",
            "Epoch 00015: loss improved from 3.64498 to 3.54634, saving model to weights-improvement-15-3.5463.hdf5\n",
            "Epoch 16/25\n",
            "99950/99950 [==============================] - 83s 830us/step - loss: 3.4497\n",
            "\n",
            "Epoch 00016: loss improved from 3.54634 to 3.44975, saving model to weights-improvement-16-3.4497.hdf5\n",
            "Epoch 17/25\n",
            "99950/99950 [==============================] - 83s 826us/step - loss: 3.3593\n",
            "\n",
            "Epoch 00017: loss improved from 3.44975 to 3.35929, saving model to weights-improvement-17-3.3593.hdf5\n",
            "Epoch 18/25\n",
            "99950/99950 [==============================] - 83s 827us/step - loss: 3.2758\n",
            "\n",
            "Epoch 00018: loss improved from 3.35929 to 3.27575, saving model to weights-improvement-18-3.2758.hdf5\n",
            "Epoch 19/25\n",
            "99950/99950 [==============================] - 82s 825us/step - loss: 3.1992\n",
            "\n",
            "Epoch 00019: loss improved from 3.27575 to 3.19924, saving model to weights-improvement-19-3.1992.hdf5\n",
            "Epoch 20/25\n",
            "99950/99950 [==============================] - 83s 829us/step - loss: 3.1225\n",
            "\n",
            "Epoch 00020: loss improved from 3.19924 to 3.12245, saving model to weights-improvement-20-3.1225.hdf5\n",
            "Epoch 21/25\n",
            "99950/99950 [==============================] - 83s 826us/step - loss: 3.0523\n",
            "\n",
            "Epoch 00021: loss improved from 3.12245 to 3.05230, saving model to weights-improvement-21-3.0523.hdf5\n",
            "Epoch 22/25\n",
            "99950/99950 [==============================] - 83s 828us/step - loss: 2.9891\n",
            "\n",
            "Epoch 00022: loss improved from 3.05230 to 2.98914, saving model to weights-improvement-22-2.9891.hdf5\n",
            "Epoch 23/25\n",
            "99950/99950 [==============================] - 82s 825us/step - loss: 2.9338\n",
            "\n",
            "Epoch 00023: loss improved from 2.98914 to 2.93385, saving model to weights-improvement-23-2.9338.hdf5\n",
            "Epoch 24/25\n",
            "95232/99950 [===========================>..] - ETA: 3s - loss: 2.8725"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EDA-LI7XqTN",
        "colab_type": "code",
        "outputId": "96e84477-3138-465a-e7ae-44289e48c031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(X)\n",
        "print(y.shape[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[919, 1308, 442, 3, 1646, 1487, 422, 7, 328, 34, 10, 1007, 1008, 9591, 7, 73, 4, 3328, 6094, 4, 7, 168, 411, 10, 3, 886, 5, 1443, 20, 166, 6, 1103, 2, 20, 14, 7, 20, 14, 7, 632, 146, 643, 9, 10118, 1188, 9, 194, 1, 371, 1]\n",
            "10241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z25PwSSWNkD7",
        "outputId": "da650ed0-73b4-493f-e4f0-aa19d06174fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "\n",
        "#seleciona um trecho aleatório do livro para servir como entrada\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "X = tokenizer.texts_to_sequences([pattern])[0]\n",
        "print('Texto fornecido como entrada: ')\n",
        "print(\"\\\"\", ' '.join([value for value in pattern]), \"\\\"\")\n",
        "print('Continuação do texto gerado pela rede: ')\n",
        "\n",
        "probability = 1.0\n",
        "\n",
        "for i in range(50):\n",
        "  x_in = numpy.reshape(X, (1, seq_length))\n",
        "  prediction = model.predict(x_in, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  probability *= prediction[0][index]\n",
        "\n",
        "  result = tokenizer.sequences_to_texts([[index]])\n",
        "\n",
        "  sys.stdout.write(result[0])\n",
        "  sys.stdout.write(' ')\n",
        "  X.append(index)\n",
        "  X = X[1:len(X)]\n",
        "  \n",
        "print()\n",
        "print('Probabilidade do texto gerado: ', probability)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Texto fornecido como entrada: \n",
            "\" comissao de agricultura e reforma agraria cra de audiencia publica para discutir programa de producao de etanol social da amazonia para tanto peco o convite d o dr marcio silveira reitor da universidade federal do tocantins e djalma bezerra mello titular da superintendencia do desenvolvimento da amazonia sudam justificacao por \"\n",
            "Continuação do texto gerado pela rede: \n",
            "diversas razoes relacionadas a questao cunha assim o plano nacional para banda larga intitulado o brasil e reportagem daeletronorte e o processo internacional para a mulher da tv senado publicado no diario do senado federal o senado federal o protocolo sera assinado em bonfim rr em 14 de junho de \n",
            "Probabilidade do texto gerado:  8.2044921546984e-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG11yXK9FZdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importando os dois outros conjuntos de dados\n",
        "\n",
        "import Pandas as pd\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}